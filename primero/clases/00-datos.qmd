---
title: "**Gestión de datos**"
format:
  html:
    page-layout: full
    toc: true
    toc-title: Contenidos
    toc-location: left
    toc_float: true
    toc_collapsed: false
    toc_depth: 4
    include-in-header: 
      - text: |
          <link rel="icon" type="image/x-icon" href="img/favicon.ico">
number_sections: true
anchor_sections: true
---

```{r, message=FALSE, echo=F}
knitr::opts_chunk$set(comment=NA, dpi = 300)
```

La gestión de datos vinculada a procesos de investigación o vigilancia epidemiológica implica la organización, el almacenamiento, la preservación y la difusión de los datos antes de iniciar la etapa de análisis final.

Los datos de los estudios de investigación incluyen los materiales generados o recopilados a lo largo de un proceso de investigación. Como podemos imaginar, esta amplia definición incluye mucho más que la gestión de conjuntos de datos digitales. También incluye archivos físicos, documentación, cuestionarios, grabaciones y más. En definitiva, es una tarea importante que comienza mucho antes de que se recopilen los datos, durante la fase de planificación, y continúa mucho después de que finaliza un proyecto de investigación durante la fase de archivo y uso compartido.

## Ciencia abierta

Con el auge tecnológico de los últimos años surgió un creciente interés en las prácticas de ciencia abierta, donde compartir datos bien gestionados y documentados ayuda a generar confianza en el proceso de investigación. Compartir datos curados de manera reproducible es *"un fuerte indicador para los colegas investigadores de rigor, confiabilidad y transparencia en la investigación científica"* según [@Alston2021]. También permite que otros repliquen y aprendan de su trabajo, validen sus resultados para fortalecer la evidencia, así como también detecten potencialmente errores en su trabajo, evitando que se tomen decisiones basadas en datos incorrectos. Compartir sus datos con suficiente documentación y metadatos estandarizados también puede conducir a una mayor colaboración y un mayor impacto ya que los colaboradores pueden acceder y comprender sus datos con facilidad.

La ciencia abierta tiene como objetivo hacer que la investigación y la difusión científicas sean accesibles para todos, lo que hace absolutamente necesaria la necesidad de buenas prácticas de gestión de datos.

Organizaciones, como el **Centro para la Ciencia Abierta** (Center for Open Science) <https://www.cos.io>, se han convertido en un conocido defensor de la ciencia abierta, ofreciendo el *Marco de Ciencia Abierta* (OSF por Open Science Framework) como una herramienta para promover la ciencia abierta a lo largo de todo el ciclo de vida de la investigación.

## Marcos existentes - FAIR

En 2016, se publicaron los Principios **FAIR** en *Scientific Data* [@Wilkinson2016], que describen cuatro principios rectores para la gestión y administración de datos científicos. Estos principios se crearon para mejorar y respaldar la reutilización de datos académicos, específicamente la capacidad de las máquinas para acceder y leer datos. Son la base de cómo se deben compartir públicamente todos los datos digitales.

Los principios son:

`F`**: Findable (Localizable)**

Todos los datos deben poder encontrarse mediante un identificador persistente y contar con metadatos completos que permitan realizar búsquedas. Estas prácticas ayudan al descubrimiento de información a largo plazo y proporcionan citas registradas.

`A`**: Accessible (Accesible)**

Los usuarios deberían poder acceder a sus datos. Esto puede significar que sus datos estén disponibles en un repositorio o a través de un sistema de solicitudes. Como mínimo, un usuario debería poder acceder a los [metadatos](https://es.wikipedia.org/wiki/Metadatos), incluso si los datos reales no están disponibles abiertamente.

`I`**: Interoperable**

Sus datos y metadatos deben utilizar vocabularios y formatos estandarizados. Tanto los humanos como las máquinas deben poder leer e interpretar sus datos. Las licencias no deben suponer una barrera para su uso. Los datos deben estar disponibles en formatos abiertos a los que se pueda acceder mediante cualquier software (por ejemplo, csv, txt, etc).

`R`**: Reusable (Reutilizable)**

Para brindar contexto para la reutilización de sus datos, sus metadatos deben brindar información sobre la procedencia de los datos, brindar una descripción del proyecto, una descripción general del flujo de trabajo de los datos y los autores a los que se debe citar para una atribución adecuada. También debe tener licencias claras para el uso de los datos.

## Fuentes de datos primaria y secundaria

El término datos primarios se refiere a datos recolectados por el investigador por primera vez. Esta situación requiere que se diseñen adecuadamente los instrumentos de recolección y la carga de datos que conlleva la necesidad de contar con una gran cantidad de recursos (tiempo, mano de obra y presupuesto).

Por otra parte, también requiere de un proceso de limpieza y depuración para tratar, previo al análisis, los datos crudos.

En cambio, las fuentes secundarias implican información que ya ha sido recopilada y registrada por otra/s persona/s diferente al analista, generalmente para un propósito que no está relacionado con el análisis actual.

Un ejemplo en el mundo epidemiológico son las encuestas nacionales de factores de riesgo (ENFR) que el *Ministerio de Salud de la Nación Argentina* en conjunto con el INDEC llevó a cabo en 2005, 2009, 2013 y 2018. Estas tablas de datos de fuente secundaria están disponibles en el sitio del [INDEC](https://www.indec.gob.ar/indec/web/Institucional-Indec-BasesDeDatos-2)

## Conceptos básicos de un conjunto de datos

Un dato contiene la mínima unidad de información y desde la óptica informática es una representación simbólica (numérica, alfabética, etc) de un atributo o característica de una entidad. Para nosotros el dato siempre representa el valor / medida (variables cuantitativas) o modalidad / categoría (variables cualitativas) de una variable.

Dentro de la investigación cuantitativa, generalmente trabajamos con datos digitales en forma de un conjunto de datos, una colección estructurada de datos.

Un conjunto de datos mínimo está organizado en un formato rectangular que permite que la información sea legible por la computadora. Los conjuntos de datos rectangulares, también llamados tabulares, se componen de columnas y filas y se asocian a una unidad de investigación u observación.

```{r}
#| echo: false
#| message: false
#| warning: false


library(tidyverse)

read_csv2("datos/datos.csv") |> head() |> knitr::kable()
```

### Columnas

Las columnas del conjunto de datos representan las variables o atributos constarán de los siguientes tipos de variables:

**Variables recopiladas**

Son variables recogidas de un instrumento (cuestionario estrcuturado, etc) en fuentes primarias o fuente secundarias (externa).

**Variables creadas**

Estos pueden ser variables construidas o pueden ser variables derivadas o indicadores con fines de resumen (llamada también información agregada), como medias, proporciones, tasas, etc.

**Variables de identificación**

También debe incluir valores que identifiquen de forma únivoca a los sujetos en sus datos (por ejemplo, el DNI o historia clínica u otro identificador ad hoc si se desea preservar la identidad).

#### Atributos de columna

Las columnas o variables de su conjunto de datos también tienen los siguientes atributos:

**Nombres de variables**

Un nombre de variable es la representación corta de la información contenida en una columna.

Los nombres de las variables deben ser únicos. Ningún nombre de variable en un conjunto de datos puede repetirse.

**Tipos de variables**

El tipo de una variable determina los valores permitidos para una variable, las operaciones que se pueden realizar en la variable y cómo se almacenan los valores.

Algunos ejemplos de tipos son numéricos, de caracteres (también llamados texto o string), de fecha o lógicos (valores `True` y `False`). Los tipos también se pueden definir de forma más específica según sea necesario (por ejemplo, continuos o categóricos).

**Valores variables**

Los valores de las variables hacen referencia a la información contenida en cada columna. En cada variable se puede configurar los valores permitidos predeterminados en forma de validación a la carga de datos.

Algunos ejemplos de configuración de valores permitidos para diferentes tipos de variables incluyen:

`Variable de caracter categórico:` “sí” \| “no”

`Variable numérica entera:` 1 a 100

`Fecha variable:` 01/10/2024 a 31/12/2024

`Variable de carácter de texto libre:` se permite cualquier valor

Cualquier cosa fuera de los valores o rangos esperados se considera un error.

**Etiquetas variables**

Una etiqueta de variable es la descripción legible por humanos de lo que representa una variable.

Esta puede ser una etiqueta con la definición de la variable o directamente la pregunta que vincula la variable con el cuestionario de origen.

### Filas

Las filas del conjunto de datos representan a los sujetos (también llamados registros u observaciones) de sus datos. Los sujetos de su conjunto de datos pueden ser personas, hogares, ubicaciones como países o provincias, etc.

### Celdas

A la intersección entre una columna y una fila la llamamos celda. En cada una de ellas puede haber solo un valor del tipo definido en la columna y que representa al sujeto de la fila.

## Bases de datos

Hasta ahora hemos estado hablando de un conjunto de datos independiente. Sin embargo, es más probable que el proyecto de investigación esté compuesto por varios conjuntos de datos, dependiendo del diseño de estudio y de las diferentes dimensiones de abordaje (por ejemplo, cuestionarios clínicos y de laboratorio). En algún momento, lo más probable es que necesitemos vincular esos conjuntos de datos.

Para pensar en cómo vincular datos, necesitamos discutir dos cosas: el diseño de la base de datos y la estructura de los datos.

::: {.callout-note appearance="simple" icon="false"}
Una base de datos es “una colección organizada de datos almacenados como múltiples conjuntos de datos”
:::

En la terminología de bases de datos, cada conjunto de datos que tenemos se considera una *"tabla"*. Cada tabla incluye una o más variables que definen de forma única las filas de sus datos (es decir, una clave principal). Las tablas también pueden contener variables asociadas con valores únicos en otra tabla (es decir, claves externas). Este juego de claves principal-externa sirve para "conectar" las tablas de forma horizontal sin perder integridad.

::: {.callout-warning appearance="simple" icon="false"}
Es importante tener en cuenta que si no tiene identificadores únicos comunes en todas las tablas, como en el caso de los datos anónimos, no podremos unir datos horizontalmente.
:::

Existe otra forma de unión de tablas que llamaremos **uniones verticales** y sucede cuando debemos anexar o apilar datos de un conjunto con otro que siempre deberá tener la misma estructura, es decir los mismo nombres de variables y tipo de datos.

## Instrumentos de recolección de datos

Las tablas de datos son producto del almacenamiento de la información que proviene muchas veces de los instrumentos de recolección más comunes en la investigación cuantitativa: **el cuestionario estructurado**.

La forma en que recibimos las variables de análisis tendrán su origen en la confección de estos cuestionarios. Por lo que la construcción de estos instrumentos de recolección nacen pensando en los componentes del dato científico.

Un dato científico esta compuesto por una unidad de observación, dimensiones de análisis que definen variables de estudio, valores o categorías conceptuales que luego se operacionalizan en indicadores que terminaran siendo los datos que encontramos en las tablas.

La unidad de observación es la entidad que deseamos estudiar, es decir, aquella que se observa para efectuar mediciones o para clasificarla en categorías. También se denomina unidad de análisis o unidad experimental y se define acompañada de un tiempo y espacio identificable.

Por ejemplo, en grandes encuestas como la ENFR tenemos varias unidades de observación: **vivienda**, **hogar**, **jefe de hogar** y **persona encuestada**.

Estas unidades de observación fueron abordadas en un espacio específico, domicilio-ciudad-departamento-conglomerado-provincia del país y en un tiempo determinado (encuestas de **2005**, **2009**, **2013** y **2018**).

Las variables o dimensiones de análisis constituyen los aspectos de las unidades de observación que se han seleccionado para examinar o estudiar, de acuerdo a los problemas e hipótesis de investigación. Concretamente son aquellos atributos, propiedades o características observables de las diferentes unidades de observación.

Por ejemplo, las variables recabadas en la ENFR son: `edad`, `sexo`, `nivel de instrucción`, `actividad física`, `consumo de alcohol`, `consumo de tabaco`, etc.

Algunas de estas variables son simples o directas como la edad donde solo hay que aclarar en que unidad se la está midiendo. Edad en años o edad en meses, etc.

Otras son constructos más complejos donde para aprehender el concepto buscado se necesitan de una serie de preguntas / variables individuales que recaban información sobre distintas dimensiones del problema. Es el caso, por ejemplo de consumo de alcohol donde podemos consultar sobre si toma, cuando lo hace, cuanto bebe, que bebe, etc.

Muchas de estas preguntas estan validadas por estudios previos y se copian con la misma estructura en diseños de cuestionarios nuevos para aprovechar la garantía de la validación y la comparabilidad que permite hacerlo.

Cuando hablamos de definiciones operacionales o indicadores vinculados a las variables conceptuales nos referimos a los procedimientos para *"medir"* a estas variables. Existen variables formuladas en términos abstractos o conceptuales con cierta complejidad que deben descomponerse en varias dimensiones y deben operacionalizarse para poder medirlas.

::: {.callout-tip appearance="simple" icon="false"}
Un ejemplo de operacionalización para la variable **Hacinamiento** es definir el indicador como:

“Cantidad de personas por cuarto, informadas por algún respondente del hogar”

-   Hasta 3 personas por cuarto es “Sin hacinamiento”
-   Más de 3 personas por cuarto es “Con hacinamiento”
:::

Por otra parte, hallamos diferentes tipos de preguntas en un cuestionario estructurado producto de este proceso operacional que va a condicionar la forma de nuestras variables digitales incluidas en la tabla de datos.

Hay preguntas cerradas con valores o códigos establecidos, preguntas abiertas que son muy difíciles de analizar en el ámbito cuantitativo o mixtas donde aparece una opción *otro/a* que se agrega a las respuestas válidas.

Tenemos también preguntas de respuesta simple, donde solo una es posible y otras de respuesta múltiple donde se puede marcar más de una respuesta simultánea. Esta situación hace que en la construcción de nuestra tabla de datos cada respuesta se refleje como una variable por *si o no*, produciendo posteriormente un análisis de mayor complejidad.

Siempre las preguntas cerradas deberán cumplir con dos cualidades particulares, ser exahustivas y excluyentes.

Exahustiva, significa que dentro de las opciones válidas predeterminadas tendremos que tener la posibilidad de abarcar todas las posibles respuestas del entrevistado. Esto provoca que muchas veces aparezcan las opciones, *"Sin dato"*, *"No sabe/No contesta"* u *"Otro/a"*.

Excluyente, es que no pueden coexistir dos respuestas simultáneamente. Si se produce, entonces habrá que construir variables de respuesta múltiple.

Junto a las preguntas y sus valores se acompaña la clasificación de variables y definición de escalas. Las variables pueden ser clasificadas en:

| Escala    | Ejemplo                                      |
|-----------|----------------------------------------------|
| Nominal   | Fuma: "si" - "no"                            |
| Ordinal   | Nivel de ingresos: "Alto" - "Medio" - "Bajo" |
| Intervalo | Hora del día: "0:00"..."24:00"               |
| Razón     | Peso (Kgrs): 0....a n                        |

Otras escalas especiales son las de [Likert](https://es.wikipedia.org/wiki/Escala_Likert), [Guttman](https://en.wikipedia.org/wiki/Guttman_scale), [visuales análogas](https://es.wikipedia.org/wiki/Escala_an%C3%A1loga_visual), etc

En muchas oportunidades encontramos en las tablas de datos provenientes de cuestionarios variables codificadas. La codificación es la tarea de asignar códigos a las distintas respuestas de las preguntas del cuestionario y obtener así los distintos valores de las variables con los que se construye la matriz de datos.

Es necesario estar atento a esta situación porque en algunas tablas de datos de fuente secundaria se suele asignar valores extremos en la escala numérica de variables con este tipo de dato, por ejemplo en la ENFR utilizan 9, 99, 999 dependiendo de los dígitos que tenga la variable para referirse a categorías tipo Ns/Nc o sin dato.

Todo lo desarrollado anteriormente nos lleva a que cada tabla de datos que construyamos nosotros mismos o las tablas provenientes de fuentes secundarias deben estar acompañadas de diccionario de datos o cuadros de operacionalización donde esten definidas las características, valores legales y codificación de cada variable de estudio.

Ver [Manual de uso de la base de datos usuario ENFR 2018](https://www.indec.gob.ar/ftp/cuadros/menusuperior/enfr/manual_base_usuario_enfr2018.pdf)

## Consideraciones éticas

Nuestras tablas de datos suelen contener información de sujetos humanos lo que conlleva a la responsabilidad de proteger a esos datos. Los datos de humanos pueden contener información identificable que aumenta el riesgo de que los participantes puedan ser revelados en un conjunto de datos. Habitualmente también contienen información sobre temas sensibles vinculado a la salud lo que aumenta aún más los riesgos si se identifica a los participantes.

Cuando se trabaja con sujetos humanos, hay dos tipos de identificadores que se pueden recopilar en el estudio: directos e indirectos.

Los identificadores directos son exclusivos de un individuo y se pueden utilizar para identificar a un participante. Los identificadores indirectos no son necesariamente exclusivos de un individuo en particular, pero si se combinan con otra información se pueden utilizar para identificar a un participante.

La [Oficina de Ética de Investigación Humana de la Universidad de Carolina del Norte](https://research.unc.edu/2020/05/01/de-identified-coded-or-anonymous-how-do-i-know/) define 4 tipos de archivos de datos en función a la identifcación personal:

1.  **Identificable**: los datos incluyen información de identificación personal. Es común que los datos crudos sin procesar de un estudio de investigación sean identificables.

2.  **Codificado**: en este tipo de archivo de datos, se ha eliminado o distorsionado la información de identificación personal y se han reemplazado los nombres por un código (es decir, un identificador único del participante). La única forma de vincular los datos con un individuo es a través de ese código. El archivo de código de identificación (clave de vinculación) se almacena por separado de los datos de investigación. Los datos codificados son, por lo general, el tipo de archivo que se crea después de limpiar los datos sin procesar del estudio (clean_data).

3.  **Desidentificado**: en este tipo de archivo, se ha eliminado o distorsionado la información de identificación y los datos ya no se pueden volver a asociar con la persona subyacente (la clave de vinculación ya no existe). Esto es lo que se crea normalmente cuando se comparten públicamente los datos de un estudio de investigación.

4.  **Anónimo**: en un conjunto de datos anónimo, nunca se recopila información de identificación, por lo que debería haber poco o ningún riesgo de identificar a un participante específico.

### Sensibilidad

Los datos suelen clasificarse en función del nivel de sensibilidad. Estos niveles de sensibilidad determinan cómo se pueden recopilar, almacenar y compartir los datos, así como cuál debe ser la respuesta ante cualquier violación de datos. Si bien existe variación, aquí se presenta un resumen general de cómo se puede categorizar la información.

-   **Sensibilidad baja**: se considera que estos datos no presentan riesgo o que presentan un riesgo bajo si se divulgan. Por lo general, esto incluye datos anónimos y no identificados que no contienen información altamente sensible.

-   **Sensibilidad moderada**: Se considera que estos datos tienen un riesgo moderado si se divulgan, lo que significa que podrían afectar negativamente a las personas. Estos datos pueden incluir información identificable o información que podría permitir volver a identificar a los participantes dentro de los propios datos o utilizando una fuente externa. Por lo general, se exige que estos datos se mantengan confidenciales por ley u otros acuerdos. Estos datos deben protegerse contra el acceso no autorizado.

-   **Alta sensibilidad**: estos datos deben estar sujetos a las medidas de seguridad más estrictas y podrían causar un gran daño si se divulgan. Estos datos incluyen información personal identificable o información que podría permitir que los participantes sean reidentificados, así como información privada o altamente sensible (por ejemplo, registros médicos) y, por lo general, se exige que se mantengan confidenciales por ley u otros acuerdos. Estos datos deben protegerse contra el acceso no autorizado.

## Plan de gestión de datos

Un plan de gestión de datos es un documento complementario sobre cómo se planea recopilar, almacenar, gestionar y compartir los productos de datos de investigación.

Es oportuno que acompañe o sea parte del protocolo de investigación o del manual de procedimiento de un programa de vigilancia epidemiológica y puede contener los siguientes items:

1. **Descripción de los datos que se compartirán**

-   ¿Cuál es la fuente de los datos? (por ejemplo, encuestas, datos existentes, sistemas de vigilancia, etc)
-   ¿Cómo se limpiarán y conservarán los datos antes de compartirlos?
-   ¿Cuál será el nivel de agregación? (por ejemplo, nivel de elemento, datos resumidos, solo metadatos)
    -   Es posible que sea necesario compartir los conjuntos de datos de un proyecto de diferentes maneras debido a razones legales, éticas o técnicas.
-   ¿Se compartirán datos tanto brutos como limpios?

2.  **Formato de los datos a compartir**

-   ¿Los datos estarán en formato electrónico?
-   ¿Se proporcionará en un formato no propietario? (por ejemplo, csv)
-   ¿Se proporcionará más de un formato? (por ejemplo, xlsx y csv)
-   ¿Se necesitan herramientas para manipular o reproducir datos compartidos? (por ejemplo, software, código)
    -   Proporcione detalles sobre esas herramientas (por ejemplo, cómo se puede acceder a ellas, número de versión, sistema operativo requerido).

3.  **Documentación a compartir**

-   ¿Qué documentación compartirás?
    -   Considere la documentación a nivel de proyecto, de conjunto de datos y de variable.
-   ¿En qué formato estará su documentación? (por ejemplo, csv, pdf)

4. **Normas**

-   ¿Planea utilizar algún estándar para cuestiones como metadatos, recopilación de datos o formato de datos?

5.  **Conservación de datos**

-   ¿Dónde se archivarán los datos internamente?
    -   Medidas de seguridad y accesos
-   ¿Se archivarán los datos para compartirlos públicamente? ¿Dónde?
-   ¿Cuáles son las características deseables del repositorio? (por ejemplo, identificadores únicos y persistentes asignados a los datos, metadatos recopilados, procedencia de los registros, opciones de licencia)
-   ¿Cuándo depositará sus datos de estudio en el repositorio y durante cuánto tiempo permanecerán accesibles los datos?
-   ¿Cómo permitirá la reutilización de datos?

6.  **Consideraciones sobre acceso, distribución o reutilización**

-   ¿Existen factores legales, técnicos o éticos que afecten la reutilización, el acceso o la distribución de sus datos?
-   ¿Se restringirán algunos datos?
-   ¿Se requieren controles de acceso (por ejemplo, un acuerdo de uso de datos)?

7.  **Protección de la privacidad y la confidencialidad**

-   ¿Los participantes firman acuerdos de consentimiento informado? ¿El consentimiento comunica cómo se espera que se utilicen y compartan los datos de los participantes? ¿Cómo evitará la divulgación de información de identificación personal cuando comparta datos?

8.  **Seguridad de los datos**

-   ¿Cómo se mantendrá la seguridad y la integridad de los datos durante un proyecto? (por ejemplo, considere el almacenamiento, el acceso, la copia de seguridad y la transferencia de datos)

9.  **Funciones y responsabilidades**

-   ¿Cuáles son los roles del personal en la gestión y preservación de datos?
-   ¿Quién garantiza la accesibilidad, fiabilidad y calidad de los datos?
-   ¿Existe un plan si un miembro principal del equipo abandona el proyecto o la institución?

## Planificación de la gestión de datos

Planificar las etapas del proceso de investigación o del protocolo de trabajo habitual en la vigilancia epidemiológica consiste en decidir y documentar los pasos a seguir por todo el equipo durante el estudio.

La reproducibilidad comienza en la fase de planificación y por lo tanto habrá que dedicar tiempo para crear, documentar y capacitar al personal en estándares de gestión de datos antes de que comience su proyecto, dado que ayuda a garantizar que los procesos se implementen con fidelidad y se puedan replicar de manera consistente.

Un ejemplo relacionado a esta planificación que es recurrente en todos los casos es la limpieza de datos.

### Plan de limpieza de datos

Un plan de limpieza de datos es una propuesta escrita que describe cómo planeamos transformar los datos sin procesar en datos limpios y utilizables. Este documento no contiene código y no depende de habilidades técnicas. Es necesario sobre todo si compartimos el trabajo con otros integrantes o bien la información se recolecta o recibe de forma periódica (por ejemplo en vigilancia epidemiológica). Dado que este documento describe las transformaciones previstas para cada conjunto de datos sin procesar, permite que cualquier miembro del equipo brinde comentarios sobre el proceso de limpieza de datos.

Un ejemplo de un plan simple de limpieza de datos para un archivo de datos cualquiera:

```{markdown}
1. Importar datos crudos
2. Visualizar datos (filas y columnas)
3. Remover registros duplicados en caso de existir (usando las reglas del caso, duplicados completos o por claves)
4. Anonimizar datos 
5. Renombrar variables basado en el diccionario de datos
6. Diagnosticar variables (tipos, inconsistencias, etc)
7. Depurar datos diagnosticados
8. Detección de valores perdidos (missing)
9. Creación de variables construidas (clasificación, cálculo, agrupamientos, etc)
10. Exportar datos limpios en el formato elegido 
```

Así como se desarrolla este plan de limpieza se definen otros planes para la recolección, análisis, comunicación, publicación, etc.

## Guía de estilo

Las guías de estilo crean estandarización dentro y entre proyectos. Los beneficios de usarlas de manera consistente mejora la interpretación de las estructuras del proyecto y organiza mejor la información, así como también aumentan la reproducibilidad e interoperabilidad.

Son muy útiles para proyectos colaborativos o integrados por muchas personas. Se pueden crear para proyectos individuales, pero también se pueden crear a nivel de equipo para que se apliquen en todos los proyectos.

### Buenas prácticas

Antes de profundizar en las partes de una guía de estilo, hay algunas cosas que debemos saber sobre cómo las computadoras leen los nombres para comprender el “por qué” detrás de algunas de estas prácticas.

1.  Evite los espacios.

-   Las operaciones de línea de comandos y algunos sistemas operativos no las admiten, por lo que es mejor evitarlas por completo. Las direcciones web URL tampoco.
-   El guión bajo (\_) y el guion (-) son generalmente buenos delimitadores para usar en lugar de espacios.

2.  Con excepción de (\_) y (-), evite los caracteres especiales en directorios y nombres de archivos.

-   Los ejemplos incluyen, entre otros ?, ., \*, , /, +, ', &, ".
-   Las computadoras asignan un significado específico a muchos de estos caracteres especiales.
-   Evite caracteres acentuados y eñes

3.  Existen varias convenciones de nomenclatura que puedes elegir para añadir a tu guía de estilo. El uso de estas convenciones te ayuda a ser coherente con los delimitadores y las mayúsculas, lo que no solo hace que tus nombres sean más legibles para los humanos, sino que también permite que tu computadora lea y busque nombres más fácilmente.

4.  Ordenado de manera útil, donde se tenga en cuenta la clasificación alfanumérica y el completado de ceros a la izquierda cuando hay más de un dígito. Utilización del estándar ISO 8601 para fechas (AAAA-MM-DD).

5.  La longitud de los caracteres es importante. Las computadoras no pueden leer nombres que superen una determinada longitud de caracteres. Esto se aplica a las rutas de archivos, los nombres de archivos y los nombres de variables.

### Estructura de directorio

Al decidir cómo estructuramos los directorios del proyecto (la organización de carpetas y archivos dentro de un sistema operativo), hay varias cosas que debemos considerar.

**Carpetas**

En primer lugar, pensemos en organizar el directorio en una estructura de carpetas jerárquica para delinear claramente los segmentos del proyecto y mejorar la capacidad de búsqueda.

Al crear la estructura de carpetas, buscamos un equilibrio entre una estructura profunda y una superficial.

-   Si es demasiado superficial, habrá demasiados archivos en una carpeta, lo cual resulta difícil de clasificar.

-   Si la ruta es demasiado profunda, se necesitarán demasiados clics para llegar a un archivo, y las rutas de archivo pueden tener demasiados caracteres. Una ruta de archivo incluye la longitud completa de las carpetas y el nombre del archivo. (por ejemplo, el límite de ruta de Windows es de 260 caracteres).

Considere establecer un límite de caracteres en los nombres de las carpetas (nuevamente para reducir los problemas al alcanzar los límites de caracteres de la ruta).

-   Haga que los nombres de sus carpetas sean significativos y fáciles de interpretar.
-   No utilice espacios en los nombres de sus carpetas.
-   Utilice (\_) o (-) para separar palabras.
-   Con excepción de (-) y (\_), no utilice caracteres especiales en los nombres de sus carpetas.
-   Sea coherente con los delimitadores y el uso de mayúsculas y minúsculas. Siga una convención de nomenclatura existente.
-   Si prefiere que sus carpetas aparezcan en un orden específico, agregue el número de orden al comienzo del nombre de la carpeta, con ceros a la izquierda para garantizar una clasificación adecuada (01\_, 02\_).

Un ejemplo de estructura de directorios completa de un proyecto podría ser el siguiente:

```{markdown}

nombre_proyecto/
├── 01_planificacion
|   ├── proyecto
|   |   ├── fuentes
|   |   |   └── ...
|   ├── reuniones
|   |   ├── minutas
|   |   |   └── ...
|   └── ...
├── 02_documentacion
|   ├── formularios
|   |   └── ...
|   ├── diccionarios_datos
|   |   └── ...
|   ├── protocolo
|   └── ...
├── 03_recoleccion_datos
|   ├── materiales
|       └── ...
├── 04_seguimiento
│   ├── cronograma
|       └── ...
├── 05_datos
│   ├── cohorte1
│   |   ├── pacientes
|   |   |   ├── encuesta
|   |   |   |   ├── datos_limpios
|   |   |   |   |   ├── archivos
|   |   |   |   |   |   └── log.txt
|   |   |   |   ├── datos_crudos
|   |   |   |   |   ├── archivos
|   |   |   |   |   |   └── log.txt
|   |   |   |   └── ...  
|   |   |   └── ... 
|   |   └── ...
|   └── ...   
└── ...


```

**Archivos**

A menudo tenemos apuro por guardar nuestros archivos y tal vez no consideramos lo poco claros que serán los nombres para los futuros usuarios (incluidos nosotros mismos).

Nuestros nombres de archivos por sí solos deberían poder responder preguntas como:

-   ¿Qué son estos documentos?
-   ¿Cuando se crearon estos documentos?
-   ¿Cuál documento es la versión más reciente?

Una guía de estilo de nombres de archivos nos ayuda a nombrar los archivos de una manera que nos permita responder a estas preguntas. Podemos tener una guía de nombres de archivos general o guías de nombres de archivos para diferentes propósitos que requieren diferentes estrategias de organización (por ejemplo, una guía de nombres para notas de reuniones de proyectos, otra guía de nombres para archivos de datos de proyectos). Repasemos varias convenciones que se deben tener en cuenta al nombrar los archivos.

-   Que los nombres sean descriptivos (un usuario debe poder comprender el contenido del archivo sin abrirlo).
-   No se debe utilizar ninguna información de identificación personal en un nombre de archivo (por ejemplo, nombre del participante).
-   Nunca utilice espacios entre palabras.
-   Utilice (-) o (\_) para separar palabras.
-   Con excepción de (\_) y (-), nunca utilice caracteres especiales.
-   Sea coherente con los delimitadores y el uso de mayúsculas y minúsculas. Siga una convención de nomenclatura existente.
-   Considere limitar la cantidad de caracteres permitidos para evitar alcanzar el límite de su ruta.
-   Formatee las fechas de forma uniforme y no utilice barras diagonales (/) para separar partes de una fecha. Es conveniente formatear las fechas utilizando la norma ISO 8601 de una de estas dos maneras: AAAA-MM-DD o AAAAMMDD
-   Al versionar manualmente los nombres de archivos, elija un indicador consistente para usar. Un método consiste es añadir un número al nombre del archivo. Con este método, considere rellenar a la izquierda los números individuales con un 0 para mantener el nombre del archivo con la misma longitud a medida que crece (v01, v02). Otro método es agregar una fecha al nombre del archivo, utilizando el estándar ISO 8601.
-   Si sus archivos necesitan ejecutarse en un orden secuencial, agregue el número de orden al comienzo del nombre del archivo, con ceros a la izquierda para garantizar una clasificación adecuada (01\_, 02\_).
-   Mantenga metadatos redundantes (información) en el nombre del archivo. Esto reduce la confusión si alguna vez mueves un archivo a una carpeta diferente o envías un archivo a un colaborador. También permite realizar búsquedas en tus archivos. Por ejemplo, coloque siempre la palabra "raw" (crudo) o "clean" (limpio) en un nombre de archivo de datos, incluso si el archivo está alojado en una carpeta "raw" o "clean".
-   Elija un orden para los metadatos del nombre del archivo (por ejemplo, proyecto -\> tiempo -\> participante -\> instrumento).

Algunos ejemplos de nombres de archivos:

```{markdown}
01_proy1_depuracion-datos_v06.R
02_proy1_modelos-ajustados_2024-09-11.R
fig01_barras-grupo_etario.png
tabla_poblacion_indec.xlsx
```

### Variables

El mismo cuidado o aplicación de las buenas prácticas vistas hasta el momento aplican en la elección de los nombres de variables de las tablas de datos.

Hay dos tipos de reglas: las que son requisitos no negociables que realmente deberían incluirse en la guía de estilo (si no se sigue estas reglas, enfrentaremos serios problemas de interpretación tanto para los humanos como para las máquinas) y las sugerencias de mejores prácticas que se recomiendan pero no son obligatorias.

**Obligatorio**

-   No nombre una variable con ninguna palabra clave o función reservada y utilizada en ningún lenguaje de programación (como if, for, repeat).
-   Establecer un límite de caracteres (La mayoría de los programas estadísticos tienen un límite de caracteres en los nombres de las variables.) Considere el equilibrio entre el límite de caracteres y la interpretación.
-   No utilice espacios ni caracteres especiales, excepto (\_). No están permitidos en la mayoría de los programas. Incluso el (-) no está permitido en programas como R y SPSS ya que puede confundirse con un signo menos. Si bien (.) está permitido en R y SPSS, no está permitido en Stata, por lo que es mejor evitar su uso.
-   No comience el nombre de una variable con un número. Esto no está permitido en muchos programas estadísticos.

**Sugerido**

-   Evite agregar acentos y eñes a las palabras que use como nombre de variable.
-   Trate que todas las tablas tengan un identificador único de observación o registro. Si no está vinculado específicamente a cada unidad de análisis, genere uno arbitrario pero que identifique cada observación (suelen usarse ID numéricos secuenciales).
-   Preste atención al uso de minúsculas y mayúsculas. La mayoría de los lenguajes de análisis son sensibles a estas diferencias.
-   Si hay variables producto de preguntas con respuestas múltiples, comience los nombres de variables asociadas con el mismo prefijo (sintomas_fiebre, sintomas_vomitos, sintomas_mialgias).
-   Separe con prefijos y sufijos los nombres de las variables que pertenecen a distintos bloques de una encuesta o a diferentes tipos de participantes. Por ejemplo en las ENFR los prefijos denotan el bloque de la encuesta, BI\_ = Bloque Individual, BH\_ = Bloque Hogar y el sufijo \_J identifica variables del jefe de hogar.

Algunos ejemplos de nombres de variables:

```{markdown}
bi_hi_01 = bloque hogar -> ingresos del hogar -> pregunta 01 
rango_edad_j = rango de edad -> jefes de hogar
nivel_instruccion_agrupado -> variable agrupada - nivel de instruccion
condicion_actividad_c -> variable construida - condicion de actividad
```

### Codificación de valores

Se pueden definir codificaciones que sigan el esquema propuesto dentro del diccionario de datos de la tabla. En algunas ocasiones, variables de interes dicotómica que luego darán lugar a análisis de regresión suelen declararse con 1 para el Si y 0 para el No, orientandose el valor al proceso de funciones de regresión logistica de lenguajes como R.

Algunas pautas a considerar en esta tarea son:

-   Los códigos deben ser únicos. Asignamos “sí” = 1 \| “no” = 0 y evitamos “sí” = 1 \| “no” = 1

-   Los códigos deben ser consistentes dentro de una variable. Para sexo asignamos “masculino” = 'm' y evitamos que “masculino” = 'm' o 'M' o 'Masculino' o 'masculino' dado que para la maquina terminarán siendo valores distintos por el uso de minúscula y mayúscula.

-   Los códigos deben ser consistentes en todo el proyecto. Si asignamos “sí” = 1 \| “no” = 0 como valor para todos los elementos de sí/no, evitamos asignar “sí” = 1 \| “no” = 0 para algunas variables y “sí” = 1 \| “no” = 2 para otras.

-   Alineamos los códigos con las opciones de respuesta lo mejor posible. Asignar “ninguno” = 0 \| “1” = 1 \| “2” = 2 \| “3 o más” = 3 y evitamos “ninguno” = 1 \| “1” = 2 \| “2” = 3 \| “3 o más” = 4

-   Los códigos de escala tipo Likert deben ordenarse lógicamente. Asignamos “totalmente en desacuerdo” = 1 \| “en desacuerdo” = 2 \| “de acuerdo” = 3 \| “totalmente de acuerdo” = 4 y evitamos “totalmente en desacuerdo” = 1 \| “en desacuerdo” = 3 \| “de acuerdo” = 4 \| “totalmente de acuerdo” = 2

Respecto a los valores faltantes hay una variedad de estilos, pero se pueden resumir en dos opciones generales:

1.  Podemos elegir dejar todos los valores faltantes en blanco.

-   La ventaja de esta opción es que no hay posibilidad de que los códigos de valores faltantes asignados (por ejemplo, -999) se confundan con valores reales. La preocupación con este método es que no hay manera de discernir si el valor realmente falta, o fue borrado por accidente u omitido durante el ingreso de datos.

2.  La otra opción es definir códigos faltantes y agregarlos a los datos.

-   Este código puede ser numérico (por ejemplo, “faltante” = 99 o -999) o de caracteres (por ejemplo, “faltante” = 'Sin dato') y puede ser un código uniforme aplicado a todos los datos faltantes, o puede ser varios códigos asignados para diferentes tipos de datos faltantes.
-   Una ventaja de este método es que elimina la incertidumbre que teníamos con las celdas en blanco. Si se completa un valor, ahora tenemos la certeza de que no se eliminó ni se omitió durante la entrada de datos.
-   Otro beneficio es que esto le permite especificar razones distintas para los datos faltantes (por ejemplo, “Sin dato” = 99, “Ilegible” = 98) si eso es importante para su estudio.
-   El mayor problema que puede ocurrir con este método es que sus códigos podrían confundirse con valores reales (si alguien no conoce la documentación sobre valores faltantes) o si usa un valor que no coincide con su tipo de variable, entonces introduce nuevos problemas de tipo de variable (por ejemplo, si se usa 'NULL' en una variable numérica, esa variable ya no será numérica) o se calculo la media de edad con valores 999 que no son reales y van a sesgar el resultado.

## Formatos de archivos

Si el proyecto de datos esta vinculado a la recolección propia, es decir a una fuente primaria de información, es recomendable que el/los archivo/s sea alguno de los formatos abiertos de texto plano con separadores. Estos archivos son universales, conocidos como valores separados por coma (csv), que utilizan separadores clásicos como (,) coma, (;) punto y coma, (tab) tabulaciones u otro simbolo a elección (por ejemplo, los archivos de la ENFR tienen la (\|) barra verticual como separador de columnas).

Las extensiones de estos archivos generalmente son `.csv` o `.txt`, o alguna otra pero el contenido siempre es texto plano que puede leerse desde cualquier *block de notas*.

En el caso que los datos provengan de una fuente secundaria es posible que el formato de los archivos sea de de un tipo específico que tendrá relación con el software donde se hizo la carga de datos o bien en el formato que los responsables de la recolección eligieron para compartir la información.

Habitualmente los archivos `.xls` / `.xlsx` (Microsoft Excel), `.dbf` (database), `.accdb` (Microsoft Access) son usados en archivos creados dentro de proyectos sencillos donde las planillas de calculo o programa de bases de datos se pueden correr en computadoras personales.

Si los datos fuern procesados previamente por algun software estadístico puede que los archivos compartidos tengan formato `.sas7bdat` (SAS), `.sav` (SPSS), `dta` (Stata) o `.RData` (R).

También podemnos llegar a encontrar archivos `.json` (acrónimo de [JavaScript Object Notation](https://es.wikipedia.org/wiki/JSON), 'notación de objeto de JavaScript') que es un formato abierto de texto sencillo o `.xml`(del inglés [e**X**tensible **M**arkup **L**anguage](https://es.wikipedia.org/wiki/Extensible_Markup_Language), traducido como 'Lenguaje de Marcas Extensible').

## Almacenamiento y seguridad

A medida que comenzamos a recopilar datos, es importante tener una estructura bien planificada para almacenarlos de forma segura y trabajar con ellos durante el estudio activo. Hay varios objetivos que debemos tener en cuenta al configurar el sistema de almacenamiento y seguridad de archivos:

-   **Seguridad:** garantizar que sus archivos no se pierdan, corrompan o editen inesperadamente.

Para cumplir con este punto se establecen formas y protocolos que se ejecuten periódicamente de copias de seguridad que pueden realizarse en discos rígidos, espacios en un servidor central o servicios online tipo Google Drive, Dropbox, etc que deberan contar con los accesos restringidos a los usuarios autorizados del proyecto.

-   **Confidencialidad:** asegurarse de que personas no autorizadas no vean ni accedan a información confidencial.

No solo estamos hablando de los archivos electrónicos producto de la recolección de datos sino también de los formularios, cuestionarios, planillas o cualquier otro dispositivo en papel que tenga información de identificación y/o sensible de participantes.

Armarios con cerraduras y espacios físicos con medidas de seguridad son recomendados en estas situaciones. Lo mismo respecto del lugar donde se encuentra la o las computadoras del equipo de trabajo, sus pendrive y discos rígidos externos.

-   **Accesibilidad y usabilidad de los archivos:** asegurarse de que el equipo de trabajo pueda encontrar los archivos fácilmente y que pueda comprender qué contienen.

Existen varias formas de lograr este objetivo y deberá evaluarse durante la planificación. Se pueden usar espacios de colaboración tipo Google Drive o proyectos privados de sistema de control de versiones como [GitHub](https://es.wikipedia.org/wiki/GitHub) o [GitLab](https://es.wikipedia.org/wiki/GitLab). Dependerá de las capacidades de los integrantes del equipo y del nivel de comodidad buscado.

## Depuración de datos

Incluso con los esfuerzos de recopilación y captura de datos mejor diseñados, los datos aún requieren al menos un poco de procesamiento adicional antes de que estén en un formato que le permita compartir con confianza. Lo que se haga en la fase de procesamiento de datos, o limpieza de datos, dependerá en gran medida de las transformaciones planificadas para sus datos, así como del nivel de garantía de calidad y procesos de control implementados durante la recopilación y captura.

En situaciones donde los datos provienen de fuentes secundarias, las hay con garatía de calidad y también sin controles como la información que sale de muchos sistemas de vigilancia epidemiológica (por ejemplo, el SNVS del SISA).

La limpieza de datos es el proceso de organizar y transformar datos sin procesar en un conjunto de datos al que se puede acceder y analizar fácilmente. Esta etapa puede dar como resultado esencialmente dos tipos diferentes de conjuntos de datos: un conjunto de datos curado para fines de intercambio de datos generales y un conjunto de datos limpio para un análisis específico.

Un conjunto de datos limpio para el intercambio significa que incluye toda la muestra del estudio (no se elimina a nadie), todos los datos faltantes siguen etiquetados como faltantes (no se realiza ninguna imputación) y no se han calculado variables específicas del análisis. Cualquier limpieza adicional se realiza en otra fase de limpieza durante los análisis.

Se puede pensar en los datos en tres fases distintas:

1.  **Datos brutos**

-   Este es el archivo sin procesar que proviene directamente de su fuente de recopilación o captura de datos. Si sus datos se recopilan electrónicamente, este es el archivo que extrae de su herramienta. Si sus datos se recopilan en papel, estos son los datos que se han ingresado en un formato legible por máquina. Si tiene una fuente de datos secundaria, este es el archivo que descarga o recibe del proveedor externo.
-   Estos datos normalmente no se comparten fuera del equipo de investigación, ya que suelen contener información identificable y a menudo es necesario procesarlos más para que un usuario final pueda descifrarlos.

2.  **Datos depurados del estudio**

-   Éste es el conjunto de datos que se puede compartir públicamente.

3.  **Datos analíticos**

-   Este conjunto de datos se crea a partir del conjunto depurado de datos (ya sea por su equipo o por otros investigadores), pero se modifica aún más para un análisis específico. Este conjunto de datos normalmente también se puede compartir públicamente en un repositorio (por ejemplo [Zenodo](https://zenodo.org/)) en el momento de la publicación para permitir la replicación del análisis asociado. Aquí aparecen las variables creadas en el análisis, posibles eliminaciones de observaciones y/o variables e imputaciones de valores perdidos.

```{mermaid}
%%| fig-align: center
flowchart TB
  A[Datos crudos] --> B[Datos limpios]
  B --> C[Datos analíticos]
```

### Criterios de calidad

Antes de limpiar nuestros datos, debemos tener un entendimiento compartido sobre cómo esperamos que se vean una vez que se depuren, que aseguren una determinada calidad.

Los siguientes son criterios posibles de limpieza:

1.  **Completo**

-   La cantidad de filas en su conjunto de datos debe coincidir con la cantidad de formularios completados registrados en su base de datos de seguimiento de participantes. Esto significa que todos los formularios que recopiló se capturaron (ya sea que se ingresaron o se recuperaron). También significa que eliminó todos los datos extraños que no pertenecen (por ejemplo, duplicados, participantes que no están en la muestra final, etc).
-   La cantidad de columnas de sus datos coincide con la cantidad de variables que tiene en su diccionario de datos (es decir, no se eliminaron variables accidentalmente). De manera similar, no debería haber datos faltantes inesperados para las variables (es decir, si se recopilaron los datos, deberían existir en su conjunto de datos).
-   Puede haber observaciones y/o variables eliminadas, pero a partir de una decisión consciente y debidamente documentada.

2.  **Válido**

-   Las variables se ajustan a las restricciones que ha establecido en su diccionario de datos (por ejemplo, tipos de variables, valores y rangos de variables permitidos, la falta de elementos a nivel de elemento se alinea con las reglas del universo de variables y los patrones de omisión definidos)

3.  **Preciso**

-   Muchas veces no hay forma de saber si un valor es verdadero o no. Sin embargo, es posible utilizar su conocimiento implícito de un participante o una fuente de datos para descubrir valores erróneos.

4.  **Coherente**

-   Los valores de las variables se miden, formatean o categorizan de manera consistente dentro de una columna (por ejemplo, todos los valores de la fecha de la encuesta tienen el formato AAAA-MM-DD).
-   En colecciones repetidas del mismo formulario, todas las variables se nombran, miden, formatean o codifican de manera consistente (por ejemplo, las categorías y/o códigos cargados en las variables son las del diccionario de datos).

5.  **Anonimizado**

-   Si se promete confidencialidad a los participantes, es necesario anonimizar los datos. En las primeras fases de limpieza, esto simplemente significa que se eliminan todos los identificadores directos de los datos y se reemplazan con códigos de estudio (es decir, identificador único del participante). Antes de compartir públicamente los datos, puede ser necesario un trabajo adicional para eliminar también los identificadores indirectos.

6.  **Interpretable**

-   Las variables se nombran para que coincidan con el diccionario de datos y esos nombres son legibles tanto para humanos como para máquinas. Según sea necesario, se agregan etiquetas de variables y valores como metadatos integrados para facilitar la interpretación.

7.  **Analizable**

-   El conjunto de datos tiene un formato rectangular (filas y columnas), legible por máquina y cumple con las reglas básicas de organización de datos.

## Softwares y lenguajes de análisis estadístico

En el ámbito cuantitativista del mundo epidemiológico se utilizan diversos paquetes estadísticos. Un paquete estadístico es un programa informático que está especialmente diseñado para resolver problemas en el área de la estadística, que estos casos aplicamos a estudios epidemiológicos.

Los paquetes más sencillos tienen interfaz gráfica por ventanas, lo que implica facilidad de uso y aprendizaje pero un mayor encorsetamiento a la hora de hacer cálculos que el programa no tenga predefinidos. Los programas más complejos exigen conocer su lenguaje de programación, pero suelen ser mucho más flexibles al poderse incluir en ellos librerías con funciones, tests o contrastes que no traen instalados por defecto.

Existen multitud de paquetes informáticos, tanto de software privado como de software libre y/o open source, dentro de los cuales encontramos:

| Interfaz gráfica (GUI) | Interfaz de línea de comando (CLI) |
|------------------------|------------------------------------|
| Excel                  | Lenguaje R                         |
| SPSS                   | Stata                              |
| EpiDat                 | Lenguaje Python                    |
| EpiInfo                | SAS                                |
| Stata                  | Lenguaje Julia                     |
| SAS                    | PSPP                               |
| Statgraphics           |                                    |
| Minitab                |                                    |
| PSPP                   |                                    |

Algunos poseen tanto interfaz gráfica como de línea de comando, aunque no llegan a ser un lenguaje de programación. Cada una con sus ventajas y desventajas, suele ser más veloz el aprendizaje de los paquetes gráficos pero también limitados a la hora de hacer cosas más complejas. La curva de aprendizaje de los lenguajes de programación como R, Python o Julia es lenta pero a su vez no tiene límites respecto del tipo de análisis que necesitemos.

## Bibliografía
